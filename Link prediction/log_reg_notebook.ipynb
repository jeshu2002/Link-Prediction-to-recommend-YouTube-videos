{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from features import topological_features, aggregate_features, get_vars, extract_features\n",
    "import pickle\n",
    "import rolx\n",
    "import numpy as np\n",
    "import utils\n",
    "import random\n",
    "\n",
    "def get_scores(train_pred, train_true, val_pred, val_true, test_pred, test_true):\n",
    "    train_accuracy = np.mean(train_pred == train_true)\n",
    "    print (train_accuracy)\n",
    "\n",
    "    train_f1 =  precision_recall_fscore_support(train_true, train_pred)\n",
    "    print (train_f1[0][1])\n",
    "    print (train_f1[1][1])\n",
    "    print (train_f1[2][1])\n",
    "    \n",
    "    val_accuracy = np.mean(val_pred == val_true)\n",
    "    print (val_accuracy)\n",
    "    \n",
    "    val_f1 =  precision_recall_fscore_support(val_true, val_pred)\n",
    "    print (val_f1[0][1])\n",
    "    print (val_f1[1][1])\n",
    "    print (val_f1[2][1])\n",
    "\n",
    "    test_accuracy = np.mean(test_pred == test_true)\n",
    "    print (test_accuracy)\n",
    "    \n",
    "    test_f1 =  precision_recall_fscore_support(test_true, test_pred)\n",
    "    print (test_f1[0][1])\n",
    "    print (test_f1[1][1])\n",
    "    print (test_f1[2][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolx(fname, fname_extended, roles=3):\n",
    "    G, dict_to_graph, graph_to_dict = rolx.load_graph_igraph(fname, fname_extended)\n",
    "    H, R = rolx.extract_rolx_roles(G, roles)\n",
    "    print(H.shape, R.shape)\n",
    "    H.tolist()\n",
    "\n",
    "    adj_mat = G.get_adjacency()\n",
    "    _, video_dict_list, graph_to_dict, neighbors, fields = get_vars(fname, fname_extended)\n",
    "    # np.save('rolx_features', H)\n",
    "    # H = np.load('rolx_features.npy')\n",
    "    \n",
    "    return adj_mat, H, video_dict_list, graph_to_dict, neighbors, fields\n",
    "\n",
    "def get_features(adj_mat, H, video_dict_list, graph_to_dict, neighbors, fields, agg_flag=False):\n",
    "    X = []\n",
    "    y = []\n",
    "    pos_data = []\n",
    "    neg_data = []\n",
    "    for row in range(adj_mat.shape[0]):\n",
    "        H_row = np.array(H[row]).flatten()\n",
    "        for col in range(adj_mat.shape[1]):\n",
    "            H_total = np.array(H[col][0]).flatten() + H_row\n",
    "            # print 'pre concatenated', type(H_total), H_total\n",
    "\n",
    "            # flag for adding into agg and topo features\n",
    "            if agg_flag:\n",
    "                local_features = extract_features(video_dict_list, graph_to_dict, neighbors, fields, row, col) \n",
    "                # skip if doesnt exist\n",
    "                if not local_features:\n",
    "                    continue\n",
    "\n",
    "                H_total = np.concatenate([H_total, local_features]) \n",
    "                # print 'after concatenated', type(H_total), H_total\n",
    "\n",
    "            if adj_mat[row][col] > 0:\n",
    "                pos_data.append((H_total, adj_mat[row][col]))\n",
    "            else:\n",
    "                neg_data.append((H_total, adj_mat[row][col]))\n",
    "    \n",
    "    return pos_data, neg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vertex Features matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/appalavenkataavinash/Downloads/CS224W-Youtube-Project-master/rolx.py:95: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  x_star, residuals, rank, s = lstsq(A, w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V is a 3356 by 485 matrix.\n",
      "Node-role matrix is of dimensions 3356 by 3\n",
      "[[0.         0.0199036  0.00744173]\n",
      " [0.         0.02791589 0.03119776]\n",
      " [0.         0.01000588 0.01839252]\n",
      " ...\n",
      " [0.         0.00740323 0.01428153]\n",
      " [0.         0.00737681 0.01464255]\n",
      " [0.         0.00737681 0.01464255]]\n",
      "[[3.97698502e+04 1.14131367e-01 2.00000000e+01 ... 1.14373638e-03\n",
      "  1.14373638e-03 2.00000000e+01]\n",
      " [6.67290721e+04 1.22706739e-01 4.00000000e+00 ... 2.08546433e-04\n",
      "  2.08546433e-04 4.00000000e+00]\n",
      " [0.00000000e+00 1.11362115e-01 3.00000000e+00 ... 1.85490526e-04\n",
      "  1.85490526e-04 3.00000000e+00]\n",
      " ...\n",
      " [3.33333333e-01 5.58823529e-01 4.00000000e+00 ... 2.06357331e-04\n",
      "  2.06357331e-04 4.00000000e+00]\n",
      " [0.00000000e+00 5.58823529e-01 4.00000000e+00 ... 1.98521122e-04\n",
      "  1.98521122e-04 4.00000000e+00]\n",
      " [0.00000000e+00 5.58823529e-01 4.00000000e+00 ... 1.98521122e-04\n",
      "  1.98521122e-04 4.00000000e+00]]\n",
      "[[0.00408449 0.         0.01162538 0.         0.         0.\n",
      "  0.         0.01162491]\n",
      " [0.00836937 0.07900764 0.09599818 0.06960625 0.04866274 0.06974306\n",
      "  0.06974146 0.09599824]\n",
      " [0.0378559  0.02855222 0.06001952 0.10154951 0.1109147  0.07429225\n",
      "  0.07429723 0.06002358]]\n",
      "Role-feature matrix is of dimensions 3 by 8\n",
      "[[0.00408449 0.         0.01162538 0.         0.         0.\n",
      "  0.         0.01162491]\n",
      " [0.00836937 0.07900764 0.09599818 0.06960625 0.04866274 0.06974306\n",
      "  0.06974146 0.09599824]\n",
      " [0.0378559  0.02855222 0.06001952 0.10154951 0.1109147  0.07429225\n",
      "  0.07429723 0.06002358]]\n",
      "(3356, 3) (3, 8)\n"
     ]
    }
   ],
   "source": [
    "fname = './dataset/0222/0.txt'\n",
    "fname_extended = './dataset/0222/1.txt'\n",
    "\n",
    "adj_mat, H, video_dict_list, graph_to_dict, neighbors, fields = get_rolx(fname, fname_extended)\n",
    "pos_data, neg_data = get_features(adj_mat, H, video_dict_list, graph_to_dict, neighbors, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vertex Features matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/appalavenkataavinash/Downloads/CS224W-Youtube-Project-master/rolx.py:95: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  x_star, residuals, rank, s = lstsq(A, w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V is a 4330 by 408 matrix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/appalavenkataavinash/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node-role matrix is of dimensions 4330 by 3\n",
      "[[0.19925109 0.16108741 0.00058692]\n",
      " [0.07929716 0.13149653 0.05354079]\n",
      " [0.19925109 0.16108741 0.00058692]\n",
      " ...\n",
      " [0.09832276 0.13102633 0.05499859]\n",
      " [0.1229468  0.13840692 0.06407499]\n",
      " [0.08100141 0.12920092 0.08111056]]\n",
      "[[1.27952413e+04 8.77933545e-02 2.00000000e+01 ... 3.99064480e-04\n",
      "  3.99064480e-04 2.00000000e+01]\n",
      " [7.14285714e-01 8.07364935e-02 1.00000000e+01 ... 2.10219785e-04\n",
      "  2.10219785e-04 1.00000000e+01]\n",
      " [1.27952413e+04 8.77933545e-02 2.00000000e+01 ... 3.99064480e-04\n",
      "  3.99064480e-04 2.00000000e+01]\n",
      " ...\n",
      " [1.06668301e+01 5.27777778e-01 1.10000000e+01 ... 2.52185559e-04\n",
      "  2.52185559e-04 1.10000000e+01]\n",
      " [1.25640720e+00 5.50724638e-01 1.20000000e+01 ... 2.52082882e-04\n",
      "  2.52082882e-04 1.20000000e+01]\n",
      " [0.00000000e+00 5.06666667e-01 9.00000000e+00 ... 1.95368248e-04\n",
      "  1.95368248e-04 9.00000000e+00]]\n",
      "[[0.         0.         0.02484668 0.         0.         0.\n",
      "  0.         0.02484638]\n",
      " [0.03682221 0.12328052 0.12870904 0.15966718 0.13157904 0.12093098\n",
      "  0.12092974 0.1287089 ]\n",
      " [0.         0.         0.         0.         0.00225488 0.\n",
      "  0.         0.        ]]\n",
      "Role-feature matrix is of dimensions 3 by 8\n",
      "[[0.         0.         0.02484668 0.         0.         0.\n",
      "  0.         0.02484638]\n",
      " [0.03682221 0.12328052 0.12870904 0.15966718 0.13157904 0.12093098\n",
      "  0.12092974 0.1287089 ]\n",
      " [0.         0.         0.         0.         0.00225488 0.\n",
      "  0.         0.        ]]\n",
      "(4330, 3) (3, 8)\n"
     ]
    }
   ],
   "source": [
    "fname_test = './dataset/080327/0.txt'\n",
    "fname_test_extended = './dataset/080327/1.txt'\n",
    "\n",
    "adj_mat_test, H_test, video_dict_list_test, graph_to_dict_test, neighbors_test, fields_test = get_rolx(fname_test, fname_test_extended)\n",
    "pos_data_test, neg_data_test = get_features(adj_mat_test, H_test, video_dict_list_test, graph_to_dict_test, neighbors_test, fields_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_balanced(pos_data, neg_data):\n",
    "    # creates positive and negative dataset for more uniform distribution of data\n",
    "    X = [pos_data[i][0] for i in range(len(pos_data))]\n",
    "    Y = [pos_data[i][1] for i in range(len(pos_data))]\n",
    "\n",
    "    random_indices = sorted(random.sample(range(len(neg_data)), len(X)))\n",
    "    X_neg = [neg_data[i][0] for i in random_indices]\n",
    "    Y_neg = [neg_data[i][1] for i in random_indices]\n",
    "\n",
    "    X.extend(X_neg)\n",
    "    Y.extend(Y_neg)\n",
    "\n",
    "    X_array = np.array(X)\n",
    "    Y_array = np.array(Y)\n",
    "    \n",
    "    print (X_array.shape, Y_array.shape)\n",
    "    from sklearn.preprocessing import normalize\n",
    "    # change this line to change the number of features\n",
    "    X_array = X_array[:, np.r_[:3]]\n",
    "    print (X_array.shape)\n",
    "\n",
    "    # runs training by splitting train/test sets\n",
    "    return train_test_split(X_array, Y_array, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(pos_data, neg_data):\n",
    "    # runs training by splitting train/test sets\n",
    "    X = [pos_data[i][0] for i in range(len(pos_data))]\n",
    "    Y = [pos_data[i][1] for i in range(len(pos_data))]\n",
    "\n",
    "    X_neg = [neg_data[i][0] for i in range(len(neg_data))]\n",
    "    Y_neg = [neg_data[i][1] for i in range(len(neg_data))]\n",
    "\n",
    "    X.extend(X_neg)\n",
    "    Y.extend(Y_neg)\n",
    "\n",
    "    X_array = np.array(X)\n",
    "    Y_array = np.array(Y)\n",
    "#     X_array = X_array[:, np.r_[:3]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_array, Y_array, test_size=0.01, random_state=42)\n",
    "\n",
    "    test_zero_vals = np.argwhere(y_test == 0)\n",
    "    test_one_vals = np.argwhere(y_test == 1)\n",
    "    print ('test zero vals', len(test_zero_vals), 'test one vals', len(test_one_vals))\n",
    "\n",
    "    zero_vals = np.argwhere(y_train == 0)\n",
    "    one_vals = np.argwhere(y_train == 1)\n",
    "    random_indices = zero_vals[sorted(random.sample(range(len(zero_vals)), len(one_vals)))]\n",
    "    random_indices = np.concatenate([random_indices, one_vals]).reshape(-1)\n",
    "\n",
    "    X_train = X_train[random_indices]\n",
    "    y_train = y_train[random_indices]\n",
    "    print (X_train.shape, y_train.shape)\n",
    "\n",
    "    train_zero_vals = np.argwhere(y_train == 0)\n",
    "    train_one_vals = np.argwhere(y_train == 1)\n",
    "    print ('train zero vals', len(train_zero_vals), 'train one vals', len(train_one_vals))\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test zero vals 112185 test one vals 443\n",
      "(83300, 3) (83300,)\n",
      "train zero vals 41650 train one vals 41650\n",
      "test zero vals 187235 test one vals 254\n",
      "(57420, 3) (57420,)\n",
      "train zero vals 28710 train one vals 28710\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = split_data(pos_data, neg_data)\n",
    "_, X_test, _, y_test = split_data(pos_data_test, neg_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest\n",
      "0.6023409363745498\n",
      "0.713970182219768\n",
      "0.3414885954381753\n",
      "0.4620032157996459\n",
      "0.8628848954078915\n",
      "0.010379945162553859\n",
      "0.35891647855530473\n",
      "0.020176384747160717\n",
      "0.44181258633839854\n",
      "0.0017657726448410804\n",
      "0.7283464566929134\n",
      "0.0035230042656916507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                              random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print ('random forest')\n",
    "train_predictions = clf.predict(X_train)\n",
    "val_predictions = clf.predict(X_val)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "get_scores(train_predictions, y_train, val_predictions, y_val, test_predictions, y_test)\n",
    "np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression\n",
      "0.5843337334933973\n",
      "0.6176204667983792\n",
      "0.4428331332533013\n",
      "0.5158222980437123\n",
      "0.7323667294100934\n",
      "0.006841126461211477\n",
      "0.4650112866817156\n",
      "0.013483881525118637\n",
      "0.6828293926577026\n",
      "0.0024368928775503344\n",
      "0.5708661417322834\n",
      "0.004853069147867996\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train, y_train)\n",
    "print ('logistic regression')\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "val_predictions = clf.predict(X_val)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "get_scores(train_predictions, y_train, val_predictions, y_val, test_predictions, y_test)\n",
    "np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_rbf\n",
      "0.5843337334933973\n",
      "0.6176204667983792\n",
      "0.4428331332533013\n",
      "0.5158222980437123\n",
      "0.7323667294100934\n",
      "0.006841126461211477\n",
      "0.4650112866817156\n",
      "0.013483881525118637\n",
      "0.6828293926577026\n",
      "0.0024368928775503344\n",
      "0.5708661417322834\n",
      "0.004853069147867996\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm.SVC(kernel='rbf')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print ('svm_rbf')\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "val_predictions = clf.predict(X_val)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_predictions, y_train, val_predictions, y_val, test_predictions, y_test)\n",
    "# np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm linear\n",
      "0.5843337334933973\n",
      "0.6176204667983792\n",
      "0.4428331332533013\n",
      "0.5158222980437123\n",
      "0.7323667294100934\n",
      "0.006841126461211477\n",
      "0.4650112866817156\n",
      "0.013483881525118637\n",
      "0.6828293926577026\n",
      "0.0024368928775503344\n",
      "0.5708661417322834\n",
      "0.004853069147867996\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm.SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print ('svm linear')\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "val_predictions = clf.predict(X_val)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_predictions, y_train, val_predictions, y_val, test_predictions, y_test)\n",
    "# np.savetxt('dataset/results.txt', test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "0.9587755102040816\n",
      "0.9246788460683647\n",
      "0.9989195678271309\n",
      "0.9603665574073219\n",
      "0.855044926661221\n",
      "0.024830969903667805\n",
      "0.9367945823927766\n",
      "0.048379575658661696\n",
      "0.8346943020657211\n",
      "0.0018151761693300055\n",
      "0.2204724409448819\n",
      "0.0036007072817874942\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print ('knn')\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "val_predictions = clf.predict(X_val)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_predictions, y_train, val_predictions, y_val, test_predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes\n",
      "0.5272388955582233\n",
      "0.5202629087856543\n",
      "0.69937575030012\n",
      "0.5966673153145772\n",
      "0.3606829562808538\n",
      "0.004074844074844075\n",
      "0.6636568848758465\n",
      "0.008099954541071453\n",
      "0.2321042834512958\n",
      "0.0017403966162806823\n",
      "0.9881889763779528\n",
      "0.0034746736437005965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print ('naive bayes')\n",
    "# makes predictions\n",
    "train_predictions = clf.predict(X_train)\n",
    "val_predictions = clf.predict(X_val)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "train_preds = [pred > 0.5 for pred in train_predictions]\n",
    "test_preds = [pred > 0.5 for pred in test_predictions]\n",
    "\n",
    "get_scores(train_predictions, y_train, val_predictions, y_val, test_predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
